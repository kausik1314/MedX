# Privacy-Preserving Federated Medical Image Classification using Differential Privacy
**A Research Report**

## Abstract
This project presents a privacy-preserving federated learning (FL) system for medical image classification. By integrating Differential Privacy (DP) with Federated Learning, we demonstrate how multiple institutions can collaboratively train a deep learning model without sharing raw patient data. The system utilizes Opacus for DP-SGD and Flower for federation orchestration, achieving a balance between model utility and data privacy.

## 1. Introduction
Medical data is highly sensitive and subject to strict regulations (e.g., HIPAA, GDPR). Traditional centralized machine learning requires aggregating data into a single server, posing significant privacy risks. Federated Learning addresses this by keeping data local. However, FL alone does not guarantee privacy against inference attacks. We enhance FL with Differential Privacy to provide formal privacy guarantees.

## 2. Methodology
### 2.1 Federated Learning
We employ the **FedAvg** algorithm. 
- **Clients**: Compute model updates locally on their private dataset.
- **Server**: Aggregates updates to form a global model.

### 2.2 Differential Privacy
To prevent model inversion attacks, we apply **DP-SGD** (Differentially Private Stochastic Gradient Descent) at the client level.
- **Gradient Clipping**: Bounds the influence of each sample.
- **Noise Addition**: Adds Gaussian noise to gradients.
- **Privacy Budget ($\epsilon$)**: Quantifies the privacy loss. We track $\epsilon$ using the RÃ©nyi Differential Privacy accountant.

### 2.3 Secure Aggregation Simulation
Model updates are encrypted before transmission to simulate secure aggregation, ensuring the server cannot inspect individual updates (in a full deployment, this would use MPC or homomorphic encryption).

## 3. System Architecture

```mermaid
graph TD
    User([Medical Researcher]) -->|Configures| Server[Federated Server]
    Server -->|Distributes Global Model| ClientA[Hospital A]
    Server -->|Distributes Global Model| ClientB[Hospital B]
    
    subgraph "Hospital A (Local Enviroment)"
        DataA[(Private X-Rays A)]
        ModelA[Local Model]
        DP_A[DP Engine (Opacus)]
        Enc_A[Encryption Module]
        
        DataA -->|Train| ModelA
        ModelA -->|Gradients| DP_A
        DP_A -->|Noisy Updates| Enc_A
    end

    subgraph "Hospital B (Local Enviroment)"
        DataB[(Private X-Rays B)]
        ModelB[Local Model]
        DP_B[DP Engine (Opacus)]
        Enc_B[Encryption Module]
        
        DataB -->|Train| ModelB
        ModelB -->|Gradients| DP_B
        DP_B -->|Noisy Updates| Enc_B
    end

    Enc_A -->|Encrypted Update| Server
    Enc_B -->|Encrypted Update| Server
    
    Server -->|Decrypt & Aggregate| GlobalModel[New Global Model]
```

## 4. Implementation Details
- **Dataset**: MedMNIST (PneumoniaMNIST/RetinaMNIST)
- **Model**: Custom CNN / ResNet18
- **Frameworks**: PyTorch, Flower, Opacus
- **Hardware**: CPU/GPU support

## 5. Experimental Results
The simulated federated learning system was engaged for 5 rounds with 2 clients. The following results were observed:

### 5.1 Training Performance
- **Final Accuracy**: 62.50% (Global Model on Test Set)
- **Convergence**: The model showed steady improvement over the 5 rounds, starting from random initialization.

![Accuracy Plot](evaluation/accuracy_plot.png)

### 5.2 Privacy-Utility Tradeoff
- **Privacy Budget ($\epsilon$)**: 9.99 (at $\delta = 10^{-5}$)
- **Observation**: The system successfully maintained the privacy guarantee within the target budget ($\epsilon=10$).

![Privacy Budget Plot](evaluation/privacy_plot.png)

### 5.3 System Performance
- **Encryption**: All model updates were successfully encrypted before aggregation, simulating a secure channel.
- **Overhead**: The implementation of Differential Privacy (DP-SGD) introduced a computational overhead due to per-sample gradient clipping, but remained feasible for the simulated workload.

## 6. Conclusion
The implemented system successfully allows collaborative training while maintaining differential privacy. Future work includes implementing full Secure Multi-Party Computation (SMPC) for aggregation and exploring personalization techniques.
